Notes on Data Structures:

0.) Big O:
n! > 2^n > n^2 > nlog(n) > n > sqrt(n) > n > 1

O(1)	=> 	Constant
O(logn)	=>  logarithmic
O(n)	=>	Linear
O(nlogn)	=>	n log-star n
O(n^2)	=>	Quadratic

1.) Arrays:
arrays are great for getting and setting when you KNOW what the index is
- contiguous block of memory
- if we know the index of an element in an array, the time to retrieve the element will be 
	the same, no matter where it is in the array
- retrieve with index => O(1)
- retrieve without index => O(n)
- add an element to a full array => O(n) 
- add an element to the end of an array(has space) => O(1)
- insert/update an element at a specific index => O(1)
- delete an element by setting to null => O(1)
- delete an element by shifting elements => O(n)

2.) Sort algorithms
2a.) stable versus unstable sorts:
- if the sort algorithm swaps duplicate items, then its an unstable sort
- while this may not matter for numbers, for objects it can lead to problems
Bubble Sort: O(n^2) complexity
	- we keep two indexes: forward current index of the array as we are sorting through it
					 : backward incrementing index of the end of the current array that we are sorting through
	- each full iteration through the array will migrate the largest value to the end of the array
	- if the value of item[a] is > value item[a+1] then swap the items
	- stable sorting
	- in-place algorithm(uses only the same array that is being sorted)
Selection Sort: O(n^2) complexity
	- same indexes as Bubble Sort
	- each full iteration through the array we keep track of the index of the largest value we have found
	  and then we swap the value at this index with the value at the end of the array
	- less swapping this way
	- unstable sorting
	- in-place algorithm(uses only the same array that is being sorted)
Insertion Sort: O(n^2) complexity
	- same indexes as Bubble Sort except the unsorted index is forward incrementing(we are migrating the
	  largest values to the front of the array)
	- sorted partition grows from left to right, unsorted shrinks from left to right
	- each full iteration through the array we keep track of the value to insert and compare it to each
	  item in the sorted partition, going from right to left
	- if the value of item to insert is < value item[a] then swap the items
	- stable algorithm
	- in-place algorithm(uses only the same array that is being sorted)
Shell Sort: O(n^2) complexity
	- variation of Insertion Sort
	- insertion sort chooses which element to insert using a gap of 1
	- Shell Sort starts out using a larger gap value
	- as the algorithm runs, the gap is reduced
	- goal is to reduce the amount of shifting required
	- unstable 
	- in-place algorithm(uses only the same array that is being sorted)
 
Recursion!!!!!!!!!!!!!!

Merge Sort: O(nlogn)
	- divide and conquer algorithm
	- recursive
	- two phases: splitting and merging
	- splitting phase leads to faster sorting during the merging phase
	- splitting is logical. we don't create new arrays
	- stable algorithm
Quick Sort: O(nlogn)
	- divide and conquer algorithm
	- recursive
	- uses a pivot element to partition the array into two parts
	- elements < pivot to its left, elements > pivot to its right
	- pivot will then be in its correct sorted position
	- in place algorithm
	- unstable
Counting Sort: O(n)
	- makes assumptions about the data
	- doesn't use comparisons
	- counts the # of occurrences of each value
	- only works with non-negative discrete values(can't work with floats, strings)
	- values must be within a certain range(has to be reasonable)
	- not an in-place algorithm 
	- unstable, we have to perform extra steps to make it stable
Radix Sort: O(n)
	- makes assumptions about the data
	- data must have some radix and width
	- because of this, data must be integers or strings
	- sort based on each individual digit or letter position
	- start at the rightmost position
	- must be a stable sort algorithm at each stage
	- radix = the number of unique digits or values that a numbering system of an alphabet has
	- example: for decimal system, radix is 10, for binary #s, radix is 2
	- width means the width of the field being sorted, so all numbers must have a consistent # of digits
Bucket Sort: O(n)
	- not in place
	- stability depends on the sorting algorithm
	- uses hashing
	- makes assumptions about the data
	- performs best when the hashed values of items being sorted are evenly distributed, so there aren't
	  many collisions
	1. distribute the items into buckets based on their hashed values(scattering phase)
	2. Sort the items in each bucket
	3. merge the buckets - can just concatenate them(gathering phase)
	- a generalization of counting sort
	- the values in bucket X must be > values in bucket X-1, and < values in bucket X+1
	
	
3.) Lists
ArrayLists:
	not synchronized(not thread safe), but faster than Vector because there is no synchronization overhead
	great for accessing(O(1)), but not so for adding or removing(O(n)) - because it is backed by an Array
Vectors:
	synchronized(thread safe), but slower than ArrayList because there is synchronization overhead
	a thread safe ArrayList 
	also backed by an Array
Singly LinkedList:
	each item is called a Node
	each Node has data, and a pointer to the next Node
	great for adding/removing(O(1)) as long as you are inserting/deleting to/from the front of the list, but not for accessing(O(n))
Doubly LinkedList:

4.) Stacks
	- LIFO data access 
	- 3 main methods of a stack: push(), pop(), and peek()
	Stacks can be based upon any abstract data type, but the time complexity of the stack depends
	on what underlying structure is being used, for example:
	O(1) for push, pop and peek when using a linked list
	O(n) for push if using an array, because it may need to be resized
	if you know the max # of items that will ever be on the stack, then an Array can be a good choice
	if memory is tight, an Array may be a good choice
	Linked List is ideal, though
5.) Queues
	- FIFO data access
	- main methods: add() or enqueue(), remove or dequeue(), peek()
	- a Deque is a double ended queue, as if it was backed by a double linked list
6.) Maps
	
Hash Tables:
	- abstract data type
	- provide access to data using keys
	- uses key/value pairs, data types don't have to match
	- optimized for retrieval(when you know the key)
	- Associative Array is one type of hash table
	- under the covers, the keys are being converted to integers, using Hashing
	- the hash function is Object.hashCode()
	- Collision occurs when more than one key has the same hashed value
	- Load Factor - tells us how full the hash table is
	- LF = # of items/capacity = size/capacity
	- LF is used to decide when to resize the underlying array backing the has table
	- LF too low = lots of empty space
	- LF too high = to many collisions
	
7.) Search Algorithms
	- only 2 real types: linear search and binary search
	
Linear Search: simple, O(n)
Binary Search: O(logn)
	- requires that the data be sorted
	
8.) Trees
	- every item in a tree is a node
	- the node at the top is the root
	- every non-root node has 1 parent
	- a leaf node has no children
	- a singleton tree has only 1 node, the root
	- each arrow between a parent node and its child is an edge
	- depth of a node: the number of edges from the node to the root
	- height of a node: the longest path to get from a node to its farthest leaf
	- height of the tree = height of the root
	- ancestors: all the parents, grandparents, etc, of a node
Binary tree:
	- every node has 0-2 children
	- children are referred to as left,right child
	- complete binary tree: all the children are as left as possible, but not all the 
	  leaves might be there, otherwise, it is a ...
	- full binary tree - all the leafs are at the bottom
Binary Search Tree: O(logn)
	- left child value < parent's value
	- right child value > parent's value
	- 4 ways to traverse a tree:
		- level : visit nodes on each level
		- pre-order : visit the root of each subtree first
		- post-order : visit the root of each subtree last
		- in-order : visit left child, root, then right child, results in sorted data
	- deleting: 3 scenarios
		- node is a leaf
		- node has one child
		- node has 2 children
9.) Heaps:
	- a Heap is a complete Binary Tree(every level of the tree is full except for the lowest level,
	  and all of the leaves on the lowest level all occupy the leftmost positions)
	- must satisfy the heap property, 2 kinds:
	- Max Heap: every parent is greater than or equal to its children
	- Min Heap: every parent is less than or equal to its children
	- binary heap must be a complete tree
	- children are added at each level from left to right
	- usually implemented as arrays, the way it works is:
		if the parent is at index i in the Array, it's children are at 2i+1 and 2i+2
		if a child is at index i, its parent is at floor((i-1)/2)
	- the max or minimum value will always beat the root of the tree
	- heapify: the process of converting a binary tree into a heap, often has to be done
	  after insertion of deletion
	- 2 step process to add:
		1. add the new Node to the bottom, left-most position
		2. Heapify: while (node's value > parent's value) --> swap node with its parent
	- no required ordering between siblings
	- Deleting:
		when deleting a node, you have to replace it with the rightmost leaf
		then, to heapify, you either heapify up or down
		- first, if the replacement value > its parent, heapify up until this is no longer true
		- else, if the replacement value < either of its children, heapify down until it isn't.
		  shouldn't matter which one you replace.
		